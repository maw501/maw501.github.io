---
layout: post
title: Variational Autoencoders - a first encounter
date: 2019-03-18
use_math: true
tags: ['VAEs', 'bayesian', 'generative modelling']
image: "vae_graph_model.png"
comments: true
---
This article will introduce VAEs, providing a synthesis of many of the excellent explanations that already exist. In particular we will derive the ELBO in 3 different ways as well as list some common questions that arose for me whilst understanding VAEs.

<!--more-->
<hr class="with-margin">
As an interesting pedagogical musing, it's simple to now point to most of the sources I originally read and can now just say "well, the answer was addressed here so you just needed to have read it more carefully". This is a key point, when we are first encountering something we are not able to spot patterns, links or appreciate the gravity of certain utterances. It's hard to resolve points readily as our working memory is already overloaded - usually it's only on the $n$th reading ($ n > 3$ for me) that things start coming together. I hope by laying out some of my confusions it will help others.

The goal in this blog is to start with an introduction to VAEs and towards the end to ask a few specific questions about VAEs and try to answer them clearly.

Please also note any errors are entirely my own and reflect my still developing understanding. If you have any additional questions please leave a message in the comments section and I'll do my best to answer or incorporate into the main article.

<hr class="with-margin">
<div class="list-of-contents">
  <h4>Contents</h4>
  <ul></ul>
</div>

<hr class="with-margin">
<h4 class="header" id="overview">Outline</h4>

There are many ways to think about VAEs and we must start somewhere. The approach taken in this post will be to introduce a VAE as a latent variable model and then link this to the neural network view of things. More of the practical aspects discussing the two approaches are mentioned in the [Q and A section](#QandA_sec_link).

<p align="center">
    <img src="/assets/img/VAEs_overview.png" alt="Image" width="750" height="600" />
</p>

<em class="figure">Rough outline of approach taken in this post</em>

<hr class="with-margin">
<h4 class="header" id="intro_VAEs">Introducing VAEs</h4>

##### A VAE is a generative model

In classical machine learning this usually means we are modelling the joint probability of $P(X, y)$ for some target $y$ and data $X$, both of which are observed - this is usually approached by modelling $P(X \| y)$ and $P(y)$. However, a VAE is a [latent variable model](https://en.wikipedia.org/wiki/Latent_variable_model) where we formulate a joint probability distribution between our data $X$ and hidden/unobserved variables $z$, which are generally multi-dimensional. We denote this joint distribution $P(X, z)$ and use the chain rule of probability to write as:

$$ P(X, z) = P(X | z) \, P(z)$$

The above equation can actually be thought of as defining a generative process.

For every data-point $i$:

1. Draw the latent variables $z_i \sim P(z)$
2. Draw a data-point $x_i \sim P(X \| z)$

For a vanilla VAE we are not thinking about the class labels $y$ but rather how to learn the data distribution $P(X)$ from which we will then be able to create new samples from. In particular we would like to be able to maximize the probability of $X$ under a generative process that assumes the latent variables, $z$. We do this by integrating/marginalising out the hidden variables $z$:

$$
P(X) = \int P(X|z; \theta) \, P(z) \, dz \tag{1}
$$

where $\theta$ represents the parameters of the distribution $P$.

There are two hard things about the above, which we will explain in more detail below:

1. Deciding how to define $P(z)$
2. Computing the integral over all $z$

<a name="prior"></a>
##### 1. Deciding how to define $P(z)$

<blockquote class="tip">
<strong>Key idea:</strong> VAEs define $P(z)=\mathcal{N}(z | \, 0, I)$ as the prior.
</blockquote>

The latent variables of a model are sometimes thought to represent intuitive aspects of our data that we expect exist but cannot observe and in other cases they may relate to more abstract concepts of our data such as categories. If our data were handwritten digits like the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) then we might imagine the latent variables encoded information such as handwriting style, stroke orientation and digit size. We could then imagine generating new digits by choosing some values of these latent variables which would correspond to an actual image.

Whilst we may like to model our data using a latent variable model often we do not want to have to manually encode the information that is contained in $z$ - furthermore, the latent variables themselves would be dependent on each other and we'd have to specify this internal latent structure. It would be better if our model was able to learn its own latent representation of the data.

VAEs make the unusual assumption that samples of $z$ can be drawn from a simple (in this case, normal) distribution. They do this by taking advantage of the fact that that it is possible to map any set of normally distributed variables to an arbitrarily complex distribution if we use a sufficiently complicated function. For VAEs we choose the sufficiently complicated function, which will model $P(X \|z; \theta)$, to be a neural network. In this case we are able to choose $P(z)$ to be a multi-dimensional isotropic Gaussian where isotropic just means the covariance matrix is the identity matrix i.e. all covariance terms are 0 so $\Sigma = I$.

We therefore set $P(z)=\mathcal{N}(z \| 0, I)$ as the prior.

See the [section](#map_rvs) below on mapping independent normal RVs to any function for a further illustration of how this works. We will make the link to neural networks and how a VAE is trained in the [section](#nn_link_sec) below.

##### 2. Computing the integral over all $z$

<blockquote class="tip">
<strong>Key idea:</strong> VAEs attempt to sample values of $z$ that are likely to have produced $X$ and use just these samples to compute $P(X)$.
</blockquote>

Conceptually we could try to compute $P(X)$ by sampling a large number of $z$ values $\\{z_{1}, \dots, z_{n}\\}$ from the prior and then calculating:

$$P(X) \approx \frac{1}{n} \sum_{i} P\left(X | z_{i}\right)$$

Unfortunately this is generally not calculable in any reasonable time due to the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality).

The way VAEs get around this is a key insight and a crucial part in understanding how they work. The idea is to observe that for most values of $z$, $P(X \| z)$ will be essentially zero, and hence contribute almost nothing to the estimate of $P(X)$. We can think of this by observing that there will only be certain values of each latent variable $z$ dimension (e.g. handwriting style, stroke orientation and digit size) that when combined with other $z$ values make a plausible looking digit for the data $X$.

Quoting [Doersch](https://arxiv.org/abs/1606.05908):

*The key idea behind the variational autoencoder is to attempt to sample values of $z$ that are likely to have produced $X$, and compute $P(X)$ just from those. This means that we need a new function $Q(z \|X)$ which can take a value of $X$ and give us a distribution over $z$ values that are likely to produce $X$. Hopefully the space of $z$ values that are likely under $Q$ will be much smaller than the space of all $z$â€™s that are likely under the prior $P(z)$.*

We can now sample $z \sim Q$ as opposed to from the prior $P(z)$. It can help to recall the original statement for $P(X)$:

<div class="math">
\begin{align*}

P(X) &= \int P(X|z; \theta) \, P(z) \, dz \\[5pt]

&= \mathbb{E}_{z \sim P} P(X | z; \theta)

\end{align*}
</div>

The introduction of $Q$ will allow us to sample $z$ more efficiently, but it's not clear yet how switching to use

$$\mathbb{E}_{z \sim Q} P(X | z; \theta)$$  

instead of  

$$\mathbb{E}_{z \sim P} P(X | z; \theta)$$

will help us maximize $P(X)$?

<blockquote class="tip">
<strong>Key idea:</strong> relating $\mathbb{E}_{z \sim Q} P(X | z; \theta)$ and $P(X)$ is a fundamental block in variational Bayesian methods.
</blockquote>

To show this relationship we move onto the famous ELBO derivation which we will derive in 3 ways.

<hr class="with-margin">
<h4 class="header" id="derive_elbo">Deriving the ELBO 3 ways</h4>

<blockquote class="tip">
<strong>Goal:</strong> Formally we will show that $\ln P(X) \geq$ ELBO and so maximizing the ELBO (which is tractable) will also be maximizing $\ln P(X)$ - our term of interest.
</blockquote>

##### Introduction

The ELBO derivation is what links the term we wish to maximize but are unable to do so directly, $P(X)$, with an equivalent expression we can solve which is tractable - it will also show the link between $P(X)$ and $Q$. How we actually compute the ELBO will be explained in the [section](#nn_link_sec) where we make the link to neural networks.

Before we start deriving the ELBO, recall that we seek a distribution $Q(z \| X)$ which will take a value of $X$ and provide a distribution over $z$ values that are likely to produce $X$. Using such a distribution we will be able to compute

$$\mathbb{E}_{z \sim Q} P(X | z; \theta)$$

which is the expectation computed over $z$ with distribution $Q$. In other words, we will be using $Q$ to approximate the true distribution of $z$ which we do not know.

There are several ways to derive the ELBO, we will show 3 variants, starting with the one shown in [Doersch](https://arxiv.org/abs/1606.05908). We will walk through the first derivation with plenty of explanation and then present 2 alternate derivations a little more succinctly.

<a name="elbo1"></a>
##### ELBO derivation 1: start with KL divergence

The start of the derivation from this angle can seem a little bit like a leap of faith without the appropriate context.

It's worth recalling that calculating the posterior of the latent variables $P(z \| X)$ is intractable (as it involves $P(X)$ which is an integral over $z$) and so variational inference attempts to find some approximate distribution that is as close to $P(z \| X)$ as possible.

In order to measure this closeness we use the [KL divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between the approximate distribution of $z$ we wish to find, $Q$, and the actual posterior of the latent variables $P(z \| X)$, which we do not know:

$$\mathcal{D}[Q(z | X) \| P(z | X)]=\mathbb{E}_{z \sim Q}[\ln Q(z | X)-\ln P(z | X)]$$

where the right-hand side is just the definition of the KL divergence (a measure of similarity between two probability distributions).

The next thing to note is that given we are using logs (maximizing the log of something is the same as maximizing the thing itself) we can use Bayes' rule to write:

$$\ln P(z | X) = \ln P(X | z) + \ln P(z) - \ln P(X) $$

where we've used the fact that $\ln (ab) = \ln(a) + \ln(b)$ and $\ln (a/b) = \ln(a) - \ln(b).$

Substituting this back in gives

$$\mathcal{D}[Q(z| X) \| P(z | X)]=\mathbb{E}_{z \sim Q}[\ln Q(z| X)- \ln P(X | z) - \ln P(z) ] + \ln P(X)$$

where we have moved $\ln P(X)$ outside the expectation as it doesn't depend on $z$. Using the linearity of expectations

$$\mathbb{E}[A + B] = \mathbb{E}[A] + \mathbb{E}[B]$$

we have (after swapping sides for a few terms)

$$\ln P(X) - \mathcal{D}[Q(z| X) \| P(z | X)] = \mathbb{E}_{z \sim Q}[ \ln P(X | z)] - \underbrace{\mathbb{E}_{z \sim Q}[ \ln Q(z| X)- \ln P(z)]}_{\mathcal{D}[Q(z| X) \| P(z)]} $$

and can note the term on the right can be rewritten as a KL term. This leaves us with

<a name="elbo_obj_term"></a>

$$\underbrace{\ln P(X)}_\text{want to max}
 - \underbrace{\mathcal{D}[Q(z|X) \| P(z | X)]}_{\geq 0} =
 \overbrace{\underbrace{\mathbb{E}_{z \sim Q}[ \ln P(X | z)]}_\text{reconstruction loss}
 - \underbrace{\mathcal{D}[Q(z|X) \| P(z)]}_\text{KL loss}}^\text{ELBO} \tag{E1}
 $$

We will maximize the right-hand side, the ELBO, as this is tractable.

Due to the fact the KL divergence is always non-negative we have shown that maximizing the ELBO is equivalent to maximizing $P(X)$ as ELBO $\leq \ln P(X)$ (because we have to add a non-negative term to the ELBO to get $P(X)$).

<blockquote class="tip">
<strong>Upshot:</strong> We can maximize the ELBO (which is tractable) instead of directly trying to maximize $\ln P(X).$
</blockquote>

<a name="elbo2"></a>
##### ELBO derivation 2: start with $\ln P(X)$

An alternative way to derive the result we desire is to start directly from the term we wish to maximize, $P(X)$ and show this is greater than the ELBO. Recall that maximizing $\ln P(X)$ is the same as maximizing $P(X)$ and also bear in mind the definition of the KL divergence in terms of expectations:

<div class="math">
\begin{align*}
\mathcal{D}(P \| Q) &= \int_{-\infty}^{\infty} P(x) \ln \left(\frac{P(x)}{Q(x)}\right) dx \\[5pt]

&=\mathbb{E}_{x \sim P}  \left[ \ln \frac{P(x)}{Q(x)} \right]

\end{align*}
</div>

<blockquote class="tip">
<strong>ELBO derivation 2</strong>
<br>
We will detail the full derivation and then provide explanation afterwards, the main body follows that shown in the <a class="reference external" href="https://www.youtube.com/watch?v=5WoItGTWV54">CS231n lecture.</a>

<div class="math">
\begin{alignat*}{1}

\ln P(X) &= \mathbb{E}_{z \sim Q} \left[\ln P(X) \right] \\[5pt]

&=\mathbb{E}_{z \sim Q}\left[\ln \frac{P(X | z) P(z)}{P(z | X)}\right]  &\text{(Bayes' rule)} \\[5pt]

&=\mathbb{E}_{z \sim Q}\left[\ln \frac{P(X | z) P(z)}{P(z | X)} \frac{Q(z | X)}{Q(z| X)}\right]  &\text{($\times$ 1)} \\[5pt]

&=\mathbb{E}_{z \sim Q}\left[\ln P(X | z) \right]-\mathbb{E}_{z\sim Q}\left[\ln \frac{Q(z |X)}{P(z)}\right] + \mathbb{E}_{z\sim Q}\left[\ln \frac{Q(z|X)}{P(z|X)}\right]  \hspace{0.25cm} &\text{(logs)} \\[5pt]

&= \mathbb{E}_{z \sim Q}\left[\ln P(X|z)\right] - \mathcal{D}\left[Q(z|X) \| P(z)\right]+\mathcal{D}\left[Q(z|X) \| P(z|X)\right] &\text{(by KL defn.)}
\end{alignat*}
</div>

and we obtain exactly the same expression, (E1), as we showed above in

<a class="reference external" href="{{page.url}}#elbo1">ELBO derivation 1.</a>

</blockquote>

The above derivation uses very similar techniques as the first derivation we saw except this time the KL terms are introduced at the end as a way to rewrite the expectations. The derivation started by rewriting $\ln P(X)$ as an expectation over $z$:

$$ \ln P(X) = \mathbb{E}_{z \sim Q} \left[\ln P(X) \right] $$

where the justification for doing this is that $\ln P(X)$ has no dependency on $z$ and so, by the linearity of expectations, $\mathbb{E}[aX] = a \mathbb{E}[X]$ we have:

<div class="math">
\begin{align*}
\mathbb{E}_{z \sim Q} \left[\ln P(X) \right] &= \ln P(X) \, \mathbb{E}_{z \sim Q} \left[ 1 \right] \\[5pt]
&= \ln P(X)
\end{align*}
</div>

<a name="elbo3"></a>
##### ELBO derivation 3: using Jensen's inequality

This derivation is perhaps the most often cited and relies on [Jensen's inequality](https://en.wikipedia.org/wiki/Jensen%27s_inequality).

<blockquote class="tip">
<strong>ELBO derivation 3</strong>
<br>Again we start from $\ln P(X)$:

<div class="math">
\begin{alignat*}{1}
\ln P(X) &= \ln \int P(X, z) \, dz \\[5pt]
&=\ln \int P(X, z) \frac{Q(z | X)}{Q(z | X)} \, dz\\[5pt]

&=\ln \left(\mathbb{E}_{z \sim Q}\left[\frac{P(X, z)}{Q(z | X)}\right]\right) \\[5pt]

& \geq \mathbb{E}_{z \sim Q}\left[\ln \left( \frac{P(X, z)}{Q(z | X)} \right) \right] \hspace{3cm} \text{(by Jensen's inequality)} \\[5pt]

&= \mathbb{E}_{z \sim Q}[\ln P(X, z)]-\mathbb{E}_{z \sim Q}[\ln Q(z | X)] \\[7.5pt]

&= \mathbb{E}_{z \sim Q}[\ln P(X | z)] + \mathbb{E}_{z \sim Q}[\ln P(z)] -\mathbb{E}_{z \sim Q}[\ln Q(z | X)] \\[7.5pt]

&= \mathbb{E}_{z \sim Q}[ \ln P(X | z)] - \underbrace{\mathbb{E}_{z \sim Q}[ \ln Q(z| X)- \ln P(z)]}_{\mathcal{D}[Q(z| X) \| P(z)]} \\[5pt]

&=  \underbrace{\mathbb{E}_{z \sim Q}[ \ln P(X | z)]
 - \mathcal{D}[Q(z|X) \| P(z)]}_\text{ELBO}

\end{alignat*}
</div>

</blockquote>


And so again we have shown that $\ln P(X) \geq$ ELBO. The above derivation as [usually presented](https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf) is a bit shorter but we have continued on a few more lines to make the link to the ELBO and our previous derivations explicit.

##### Final notes
The above derivations, whilst useful from a probabilistic perspective, do not make clear the links to neural networks and how we are going to end up with a VAE as an encoder-decoder model trained via stochastic gradient descent - though it does provide a sound theoretical basis.

<blockquote class="tip">
<strong>Note:</strong> ELBO stands for Evidence Lower BOund and refers to the fact the ELBO provides a lower bound for $ \ln P(X)$, the log probability of the observed data.
</blockquote>

<a name="nn_link_sec"></a>
<hr class="with-margin">
<h4 class="header" id="nn_link">Making the link to neural networks</h4>

So far we've motivated and explained VAEs from the perspective of a probability model. However, the most common encounter for non-statisticians with VAEs is likely to be through a neural network model with a different set of terminology and way of thinking. The [original article](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/) I read on this topic is an excellent read (note there are notational differences with this post) and in the section below we will expand on the content from this post, bringing into play wider sources.

<p align="center">
    <img src="/assets/img/encoder-decoder.png" alt="Image" width="600" height="400" />
</p>

<em class="figure">Basic structure of a VAE model</em>

<blockquote class="tip">
<strong>Key idea:</strong> A VAE takes in data $X$ and via an encoder, learns a hidden representation of the data $z$. It then samples from this learned latent space to recreate the original data via the decoder. The whole model is trained end to end via back-propagation.
</blockquote>

##### The encoder

The encoder is a neural network which takes the data $X$ as input and outputs parameters, $\phi$, to $Q(z \| X)$. If our data were MNIST then we'd have images of size 28 x 28 pixels and so the data $X$ would exist in 784 dimensions. The encoder will 'encode' this data into a much smaller latent space, say, of only 10 dimensions. These 10 new dimensions are the parameters for $Q(z \| X)$ and if this is a Gaussian probability density then we would need to output 2 vectors of length 10 from the encoder: one representing the mean and the other the variances of $Q(z \| X)$, which we can call $\mu_{z\|X}$ and $\sigma_{z\|X}^2$.

In this way the latent space is stochastic - it is represented by a probability distribution from which we can sample to get noisy representations of the data.

How do we train this part of the model?

This is done by comparing the latent space representation learned by the model to a prior of what we want this latent space to look like. The loss function for this part of the model is a measure of similarity (using the KL divergence) between our learned $Q(z \| X)$ and the prior for $z$, which we stated [above](#prior) is an isotropic Gaussian. For a VAE the encoder loss term is:

$$\mathcal{D}[Q(z|X) \| P(z)]$$

This is one part of the ELBO equation which we derived above and forms half of the overall loss function we use when training a VAE. When we derived the ELBO above we didn't really explain any intuition about why having this KL term in the objective function helps, some explanation about this is given in the Q and A section [below](#kl_obj).

##### The decoder

The decoder is also a neural network. It takes as its input the latent space representation of our data $z$ (sampled from Q with parameters $\mu_{z\|X}$ and $\sigma_{z\|X}^2$) and outputs a reconstruction of the data, $\tilde{X}$.

\[Note: Technically this isn't quite right and represents a disconnect between the theory and what is done in practice, see Q and A explanation [below](#decoder_output).\]

The decoder learns how to take $z$ and reconstruct the data - this probability distribution is denoted by $P(X \| z)$. Given the decoder is taking data from a lower dimensional latent space and outputting $\tilde{X}$ with the same dimensionality as $X$ then we expect some information loss. This information loss is captured by the loss function of the decoder:

$$\mathbb{E}_{z \sim Q}[ \ln P(X | z)]$$

which is the other part of the ELBO loss function we derived above. This loss computes how well the decoder has learned to reconstruct an input data-point $x_i$ given its latent representation $z_i$. In practical terms this is usually a loss function like BCE or MSE applied to $X$ and $\tilde{X}$ and which exact loss function you use depends on your data.

##### The full neural network model

As shown above, the loss function, $\mathcal{L}$, the VAE minimizes is given by the negative of the ELBO. This maximizes the ELBO, which in turn maximizes $P(X)$.

$\mathcal{L}$ is given by:

$$\mathcal{L} =
 \overbrace{\underbrace{ - \mathbb{E}_{z \sim Q}[ \ln P(X | z)]}_\text{reconstruction loss}
 + \underbrace{\mathcal{D}[Q(z|X) \| P(z)]}_\text{KL loss}}^\text{-ve ELBO} \tag{2}
 $$

It's important to note that the weights and biases of the encoder and decoder neural networks are not shown here and are what allow the encoder and decoder to learn the distributions $Q$ and $P$. It is possible to have as many hidden layers as desired in the encoder and decoder so long as the output dimensions tie-up (though perhaps not advised, VAEs are hard to train practically).

It is now possible (with one [trick](https://www.quora.com/What-is-the-reparameterization-trick-in-variational-autoencoders), not discussed here) to train the whole VAE on a GPU using back-propagation and stochastic gradient descent, which is fast and highly scalable.

<hr class="with-margin">
<h4 class="header" id="notation">Notation summary</h4>

Statistics is notorious for notational inconsistency and given I'm collating references I'm conscious that I may be guilty of this myself. In order to aid understanding, here is a summary of most of the notation used in the post:

* $P(X)$: the true probability distribution of the data, we wish to maximize this.
* $P(z)$: the prior distribution of the latent variables. For some reason it seems to be convention to use lower case for $z$ and so we will keep that up throughout this post even though $z$ is a random variable.
* $Q(z \| X)$: the probability distribution over latent variables given data $X$. This will be learned by the encoder which has parameters $\phi$ and so is sometimes written as $Q_{\phi}(zâˆ£X)$ or $Q(zâˆ£X; \phi).$ For a Gaussian distribution we can denote these parameters $\mu_{z\|X}$ and $\sigma_{z\|X}^2$.
* $P(X \| z)$: the probability distribution over the data given latent variables $z$. This will be learned by the decoder which has parameters $\theta$ and so is sometimes written as $P_{\theta}(Xâˆ£z)$ or $P(Xâˆ£z; \theta).$ For a Gaussian distribution we can denote these parameters $\mu_{X\|z}$ and $\sigma_{X\|z}^2$.
* $\tilde{X}$: the reconstructed data from the decoder.
* In general we will be referring to the data $X$ and latent variables $z$ in totality, if we refer a single data-point $i$ this will be indexed by $x_i$ and $z_i$.
* It's often (sloppily) said that $Q$ is the encoder and $P$ is the decoder - really it is meant that the encoder and decoder output parameters to the distributions $Q$ and $P$. We will clarify this further later and make the connection to the neural network language of encoders and decoders.
* The **;** semi-colons you see being used above is notation to distinguish between different types of inputs, usually input variables and parameters.

<hr class="with-margin">
<h4 class="header" id="ref">References</h4>

Here are some of the main references I used when reading about VAEs. Most of the questions I had when starting to understand VAEs were based on having read the below and trying to synthesize my knowledge:

* [Tutorial - What is a variational autoencoder?](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)
  * An excellent article by a student of David Blei which links the NN perspective of VAEs to that probabilistic interpretation (via graphical models). This was my first reading on the subject and is an accessible introduction.
* [Variational Autoencoders](https://www.jeremyjordan.me/variational-autoencoders/)
  * Another excellent article with some great images and explanations of latent variable models.
* [Tutorial on Variational Autoencoders](https://arxiv.org/abs/1606.05908) by Carl Doersch
  * A widely cited reference that "introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior".
* [Notes on Variational Autoencoders](http://www.1-4-5.net/~dmm/ml/vae.pdf) by David Meyer
  * This article is dated before the above tutorial and contains some of the exact same wording - I'm unclear which came first but there are some nice pictures in the article.
* Lecture 13 from the 2017 Stanford course CS231n on generative modelling, slides [here](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf) and video [here](https://www.youtube.com/watch?v=5WoItGTWV54).
  * A great overview with some really well put together slides, like the rest of the course.
* [Variational Autoencoders](http://bjlkeng.github.io/posts/variational-autoencoders/) by Brian Keng
  * A long and detailed blog post with two follow ups [here](http://bjlkeng.github.io/posts/a-variational-autoencoder-on-the-svnh-dataset/) and [here](http://bjlkeng.github.io/posts/semi-supervised-learning-with-variational-autoencoders/) which is extremely well written and like all of Brian's posts form an excellent base for digging deeper into a topic.
* [Density Estimation: Variational Autoencoders](http://ruishu.io/2018/03/14/vae/) by Rui Shu
  * A very well written blog with some excellent tips on the issues faced when training VAEs.
* [Auto-Encoding Variational Bayes](https://arxiv.org/abs/1312.6114) by Diederik P Kingma, Max Welling
  * The original paper introducing VAEs
* [Semi-Supervised Learning with Deep Generative Models](https://arxiv.org/abs/1406.5298) by Diederik P. Kingma, Danilo J. Rezende, Shakir Mohamed, Max Welling
  * A paper introducing VAEs in a semi-supervised learning fashion.

<a name="QandA_sec_link"></a>
<hr class="with-margin">
<h4 class="header" id="QandA">Appendix: Q and A section</h4>

This section lists some of the questions that came up for me whilst learning about VAEs and trying to implement then on different datasets to the standard MNIST example - I also give my attempts at an answer.

<a name="overload"></a>
### Why is there so much notational overload?

Tell me about it.

You need to remember that the convention in VAEs is to call the encoder $Q$ and the decoder $P$.

However **$P$ is an overloaded term**, don't get confused. Statisticians have a habit of calling any probability distribution they don't know $P$, and sometimes they use lower-case too. For even more notational abuse, just think of Bayes' theorem...where each $P$ actually means a potentially different distribution!

Recall a probability distribution is a mathematical function which (generally) has some parameters. Those parameters themselves can be modelled by complicated functions which can lead to notational confusion. For example, consider the following expression for the data likelihood:

$$ P(Xâˆ£z; \theta) = \mathcal{N}(X| \, f(z;\theta), \, \sigma^2 * I)$$

In the above $P$ is now being used in a more general sense to just mean a probability distribution and the function $f$ is actually the decoder neural network which in this case outputs (only) the means for the probability distribution $P$. I'm highlighting this point as this is the actual notation often used simultaneously in the same paper and it's important to get straight what is going on.

<blockquote class="tip">
<strong> Note </strong>
<br>
Technically the encoder and decoder output parameters for a distribution which we then sample from to get either $z|X$ or $X|z$. However, confusingly, we generally don't sample from $ P(Xâˆ£z; \theta) $ to get the reconstructed data, $\tilde{X}$. Thus sometimes people refer to the decoder as actually outputting the data $X|z$ and sometimes to outputting the parameters to $ P(Xâˆ£z; \theta) $ - keep this in mind for now but don't dwell on it. For both $z|X$ or $X|z$ we eventually will have distributions whose parameters are modelled by complicated functions with their own parameters (i.e. neural networks with weights and biases).
</blockquote>

<a name="enc_dec"></a>
#### What is the difference between the encoder, decoder, inference network and generator network?

* **Encoder or inference network**: these are two terms for the same thing. The encoder takes as input the data $X$ and trains a neural network with parameters denoted by $\phi$, outputting parameters to $Q_{\phi}(zâˆ£X)$ which is a probability density function. Thus if $Q$ is a Gaussian distribution, the encoder neural network will learn a vector of means and variances whose length depends on the dimensionality of the latent variables (we choose this).
* **Decoder or generative network**: these are two terms for the same thing. The decoder takes as input latent variables $z$ and trains a neural network with parameters denoted by $\theta$, outputting parameters to $P_{\theta}(Xâˆ£z)$ which is a probability density function. Thus if $P$ is a Gaussian distribution, the decoder neural network will learn a vector of means and variances whose length depends on the dimensionality of the original data.

<a name="graphical_model"></a>
#### Why does the VAE graphical model notation only show the generator?

Commonly we see the graphical model for a VAE specified as something like below which describes the decoder/generative network only:

<p align="center">
    <img src="/assets/img/vae_graph_model.png" alt="Image" width="250" height="300" />
</p>

<em class="figure">Plate notation for VAEs. The N denotes the number of times we sample with the parameters fixed.</em>

Where is the encoder?

In somewhat circular feeling reasoning this is because this is the really the graphical model we assume. This model structure doesn't tell us how to learn $z$, only that it is assumed such $z$ exist.

Two points on this:
* VAEs learn $z$ via the encoder/inference network.
  * Technically they actually learn $z\|X$, this is mentioned [above](#enc_dec).
* Loosely speaking the ELBO derivation shows (amongst other things) us that learning $z$ via an encoder network we are able to still maximize the term we are interested in, $P(X)$.

<a name="variational"></a>
#### Why does the term variational appear in the name?

Because we are performing variational inference, which means we are using an approximation to the posterior of interest rather than trying to calculate the exact posterior directly. Recall this posterior is the output of the encoder and is over the latent variables, i.e. $Q(z \| X) $.

I quite liked [this](https://www.quora.com/What-is-variational-inference) quote:

*In short, variational inference is akin to what happens at every presentation you've attended. Someone in the audience asks the presenter a very difficult answer which he/she can't answer. The presenter conveniently reframes the question in an easier manner and gives an exact answer to that reformulated question rather than answering the original difficult question.*

<a name="dist_assumptions"></a>
#### What distributional assumptions are we making about the data?

With all the talk of Gaussians it's tempting to think VAEs place restrictive distributions on the data we wish to model. This is not the case.

<blockquote class="tip">
<strong>Short answer:</strong> the data (independent of the model) is given by $P(X)$ and VAEs generally only place distributional assumptions on $P(X|z; \theta)$, and $P(z)$.
</blockquote>

The longer answer starts with the fact that our encoder will be outputting parameters to a distribution $Q(z\|X)$ which is being pulled towards the prior, $P(z)$, which is an isotropic Gaussian. The data distribution $P(X)$ can be arbitrarily complex and it is known (trust me on this or see the question [below](#map_rvs)) that we can map any set of normally distributed variables to an arbitrarily complex distribution if we use a sufficiently complicated function. The 'sufficiently complicated function' we use is a neural network, which are known to be [universal function approximators](http://neuralnetworksanddeeplearning.com/chap4.html).

Note that the data likelihood can be expressed as:

$$P(Xâˆ£z; \theta) = \mathcal{N}(X| \, f(z;\theta), \, \sigma^2 * I)$$

where $f$ is the neural network that allows us to learn the 'sufficiently complicated function' which models the mean of the data likelihood. To illustrate the distributional point that $P(X\| \, z; \theta)$ can be Gaussian even when $P(X)$ clearly isn't, consider the chart below. Here the data $P(X)$ clearly isn't multivariate normal as the marginal distributions are bimodal in both the the $x_1$ and $x_2$ dimensions. However, it is not difficult to see that given a cluster assignment the data within a cluster is multivariate normal.

Thus $P(X\|z)$ in this case is now Gaussian, where $z_i$ would denote a per observation cluster assignment

<p align="center">
    <img src="/assets/img/simple_cluster.png" alt="Image" width="500" height="400" />
</p>
<em class="figure">Illustration of data being Gaussian conditioned upon cluster assignment.</em>

<a name="map_rvs"></a>
#### Mapping independent normal RVs to any function (illustration)

To see that we can go from independent normal random variables (RVs) in 2d to any complicated function consider the example below, from [Tutorial on Variational Autoencoders](https://arxiv.org/abs/1606.05908) by Carl Doersch.

We start with data samples from a Gaussian in 2 dimensions (i.e. these could be the latent variables, $z$) and these samples are then mapped through the function:

$$f(z) = \dfrac{z}{10} + \dfrac{z}{||z||}$$

to form a ring. This is how VAEs can model arbitrarily complex distributions for the data, the deterministic function $f$ is a NN learned from the data(!) - the decoder.

<p align="center">
    <img src="/assets/img/2d_normal_RVs.png" alt="Image" width="800" height="400" />
</p>
<em class="figure">Creating data of an arbitrary distribution from normally distributed $z$.</em>

If you wish to verify this yourself, here is some code showing the above data generation:

<pre><code class="language-python">import numpy as np
X = np.random.multivariate_normal([0, 0], [[1, 0], [0, 1]], size=(250))

def euc_norm(X):
    '''Computes euclidean norm of array along axis 1'''
    return np.expand_dims(np.sqrt(np.sum(X**2, 1)), axis=1)

def arb_func(X):
    '''Arbitrary transformation'''
    return X/10 + X/euc_norm(X)

out = arb_func(X)
</code></pre>

<a name="kl_obj"></a>
#### Why do we need a KL loss term in the objective function?

The intuition surrounding this explanation is very interesting and worth the effort to understand. It also relies on knowing the final form of the term we will maximize (the ELBO) - if you aren't familiar with this you can see it in the [section](#elbo_obj_term) where we derive the ELBO.

<blockquote class="tip">
<strong>Short answer:</strong> the KL loss forces the encoder to distribute the latent representations for each $x_i$ around the centre of the (high-dimensional) latent space. In other words it stops the encoder outputting a different $\mu$ and $\sigma$ for every observation $i$ which would result in a non-smooth latent state representation and would be troublesome to sample from.
</blockquote>

A key benefit from using a VAE is to learn a smooth latent state representation of the input data from which we can sample.

Recall that what we output from the encoder are parameters $\mu_{z\|X}$ and $\sigma_{z\|X}^2$ which have a dimensionality equal to what we specify as the latent dimension. As the encoder is learning $Q(z\|X)$ then the parameters we get back will depend on the $X$ we put in. It is possible then that the encoder could just learn to output different $\mu_{z\|X}$ and $\sigma_{z\|X}^2$ for every example $x_i$. Such a latent space representation would be non-continuous and thus hard to sample from.

Having $z$ depend on $X$ is what we want - it allows us to avoid the curse of dimensionality by allowing us to sample only from the $z$ that matter and makes the posterior tractable.

The chart below shows 3 cases:

<p align="center">
    <img src="/assets/img/vae_latent.png" alt="Image" width="750" height="300" />
</p>
<em class="figure">Latent space representations for MNIST, [image credit](https://www.jeremyjordan.me/variational-autoencoders/).</em>

* The left image is if we only used the reconstruction loss when training the VAE (as in a normal autoencoder). In this case the latent representations have gaps in them and are clustered apart.
* The middle image shows what happens if we just use KL loss: the encoder is now essentially outputting $\mu_{z\|X} = 0$ and $\sigma_{z\|X}^2 = 1$ regardless of what $X$ we feed into the model.  In other words, irrespective of what an observation looks like, we encode it the same; and so we've failed to describe the original data.
  * This is closely related to the phenomenon of 'posterior collapse' which is explained [below](#post_collapse).
* The right hand image is when we use both reconstruction and KL loss as in a well-trained VAE - the model learns latent states for observations with distributions close to the prior, $z \sim \mathcal{N}(0, I)$, but deviating when necessary to describe salient features of the input.


For further reading on this topic consult the fantastic posts [here](https://towardsdatascience.com/intuitively-understanding-variational-autoencoders-1bfe67eb5daf) and [here](https://www.jeremyjordan.me/variational-autoencoders/) which I have used as reference.

<a name="mean_field"></a>
#### What is mean-field and amortized inference?

This is perhaps the area of VAEs that is least clearly explained in my view.

Recall that in a VAE we are approximating the true (unknown) posterior of the latent variables, $P(z \| X)$ with a distribution output from the encoder, i.e. $Q_{\phi}(zâˆ£X)$.

Bluntly speaking, in order to learn an approximate posterior (to get the $z_i$ when we sample) we need to learn some parameters. Broadly speaking there are two ways this can go:

1. **mean-field variational inference:** we can have parameters for each observation $i$ such that as the data grows the number of parameters we need to estimate grows.
2. **amortized variational inference:** we share ('amortize') the parameters we learn across many data points.

VAEs use amortized inference as the encoder (which is a NN) has a fixed number of parameters, $\phi$, which do not change in number as we scale the data. It is important not to get confused here: we sample from the learned encoder distribution $Q_{\phi}(zâˆ£X)$ enough times to obtain a $z_i$ for each data point $x_i$ but the number of parameters we learn in order to output the mean and variances for $Q$ is fixed.

In other words amortized variational inference allows us to use a parameterized function that maps from the observation space of the data to the parameters of the approximate posterior distribution. The encoder neural network accepts an observation as input, and outputs the mean and variance parameter for the latent variable associated with that observation. We are then able to optimize the parameters of this neural network instead of the individual parameters of each observation.

##### Do we give up anything by using amortized variational inference?

Yes, the model is now less expressive as in addition to making the approximate posterior Gaussian, we now are imposing an additional constraint by using a NN which is sharing parameters (rather than having a parameter for every data point). This cost is known as the [amortization gap](https://arxiv.org/pdf/1801.03558.pdf). For a NN with infinite capacity, this gap would go away but this is not the case in any practical implementations (as all networks have finite capacity).

More reading [here](https://www.quora.com/What-is-amortized-variational-inference), [here (section 6)](https://arxiv.org/pdf/1711.05597.pdf) and [here](http://bjlkeng.github.io/posts/variational-bayes-and-the-mean-field-approximation/).

<a name="iso_gauss"></a>
#### Why do VAEs assume $p(z)$ is an isotropic Gaussian - isn't this restrictive?

<blockquote class="tip">
<strong>Short answer:</strong> No, due to the fact that we can map any set of independent normally distributed variables to an arbitrarily complex distribution if we use a sufficiently complicated function.
</blockquote>

This is also answered [here](#map_rvs).

The longer answer (there is one) is beyond the scope of this blog post and my present knowledge.

<a name="decoder_output"></a>
#### What is the output of the decoder?

\[related: what if I have real-valued data, which loss function should I use?\]

This point confused me when training a VAE for a different dataset with real-valued output (i.e. numeric data).

Technically the decoder outputs parameters to $P_{\theta}(Xâˆ£z)$ (the data likelihood) and in order to generate new data $X$ we should sample from this distribution as shown below.

<p align="center">
    <img src="/assets/img/vae_model_cs231n.png" alt="Image" width="750" height="300" />
</p>
<em class="figure">VAE model, image from CS231n, lecture 13 2017, [here](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf).</em>

However, if you've ever implemented a VAE you'll realise that no-one actually performs that sampling of $X$ from $P_{\theta}(Xâˆ£z)$ - this was a source of considerable confusion initially for me. Instead the decoder simply returns the approximate data $\tilde{X}$ as samples from the generator which is implicitly the same as returning the mean $f(z;\theta)$ from the data likelihood:

$$\tilde{X} = P(Xâˆ£z; \theta) = \mathcal{N}(X | \, f(z;\theta), \, \sigma^2 * I)$$

You can read more about this practice [here](http://ruishu.io/2018/03/14/vae/).

If you are inputting real-valued numeric data (i.e. simple structured data from a csv) you can use MSE as a loss function for the decoder. If you do this be sure to centre and scale the data appropriately and remove any sigmoid function from the end of the decoder.

<a name="post_collapse"></a>
#### What is posterior collapse and should I be scared of it?

Yes, probably.

This is an open area of research which I stumbled across when trying to train a VAE on a different dataset to what is provided in the standard tutorials. It's (apparently) well-known within the research community that VAEs are hard to train and suffer from a few practical problems. However, given most blog posts either don't provide practical implementations or use a simple MNIST dataset it's easy to not realise these issues exist.

<blockquote class="tip">
<strong>Short answer:</strong> Posterior (or component) collapse is when the KL term from the encoder falls to 0 whilst training the VAE - this implies that $Q(z|X)$ is equal to the prior $P(z)=\mathcal{N}(z | 0, I)$ regardless of what $X$ is.
</blockquote>

Why is it a bad thing that $Q(z\|X)$ equals the prior - isn't this what we are trying to achieve by minimizing the KL divergence between the two terms?

No. We wanted to learn a distribution $Q$ over $z$ values that depended on $X$ (hence are likely to have produced $X$) in order to save ourselves searching the whole $z$ space. If the output of the encoder is always the prior (i.e. $\mu_{z\|X} = 0$ and $ \Sigma_{z\|X} = 1$) regardless of what $X$ we feed in, how is the decoder going to decode these $z$ into reconstructed $X$ samples? It won't be able to and we will get nonsense out of the decoder.

The idea of minimizing the KL term $\mathcal{D}[Q(z\|X) \| P(z)]$ isn't to drive it to 0 but rather to use the prior as a regulariser on the structure of the latent space the encoder learns when it outputs parameters to $Q(z\|X)$. See the related question [here](#kl_obj).

##### How can we solve posterior collapse?

I'm not going to provide a full answer to this, principally because it's an open area of research with no clear resolution. I will however point in the direction of resources I found useful when resolving the issue:

* [Variational Autoencoder and Extensions](http://videolectures.net/deeplearning2015_courville_autoencoder_extension/) by Aaron Courville
  * Skip to ~33 minutes to see the discussion on component collapse.
* [Lagging Inference Networks and Posterior Collapse in Variational Autoencoders](https://arxiv.org/abs/1901.05534) by Junxian He, Daniel Spokoyny, Graham Neubig, Taylor Berg-Kirkpatrick
  * A very recent paper discussing this topic though I didn't find their solution to work.
* [Preventing Posterior Collapse with delta-VAEs](https://deepmind.com/research/publications/preventing-posterior-collapse-delta-vaes/) by Deepmind

<hr class="with-margin">
