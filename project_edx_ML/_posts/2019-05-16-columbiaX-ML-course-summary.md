---
layout: post
title: Machine Learning - course overview (read first)
date: 2019-05-16
use_math: true
image: "ml_main_cropped.png"
comments: true
tags: [summary]
---
Please read for an overview of course notes written based on the [edX ColumbiaX ML course](https://www.edx.org/course/machine-learning-columbiax-csmm-102x-4). The notes represent my take on the content and were a fun way to explore the material deeper than the course allowed.

<!--more-->
<hr class="with-margin">
<div class="list-of-contents">
  <h4>Contents</h4>
  <ul></ul>
</div>

<hr class="with-margin">
<h4 class="header" id="overview">Overview</h4>

<blockquote class="math">
<strong>Update</strong>
<br>
The course notes are in the process of being updated to make stand-alone.
<br>
<br>
Target completion date: mid-June 2019
</blockquote>

##### Content

The story we will follow will be that of the ColumbiaX course however whilst the goal is to follow the core direction of the course we will deviate in some parts.

We will make wider use of external references and examples that are freely available to both bring content to life and to explain some of the trickier parts.

In particular, where we present mathematical derivations we will not skip details (as is often the case in machine learning courses) but instead break calculations down and reference known results used where possible either in a mathematical results section or an appendix.

We will also provide practical examples using Python code where applicable.

A big part of the course is to ground a lot of common machine learning models in a probabilistic modelling framework.

Topics covered include: classification and regression, clustering methods, sequential models, matrix factorization, topic modeling and model selection.

There are many algorithms covered, including: linear and logistic regression, support vector machines, tree classifiers, boosting, maximum likelihood and MAP inference, EM algorithm, hidden Markov models, Gaussian processes, Kalman filters, k-means, probabilistic PCA, Gaussian mixture models and more.

##### Nature of the course and assumed background

This is a primarily theoretical course and it is generally assumed that the reader has met most of the algorithms and ideas in the course before and is wanting to explore further the theory and mathematics behind machine learning. As such we won't typically dwell on motivating why or when a particular algorithm is of use.

Mathematical knowledge required is key concepts and results from linear algebra, probability and statistics and calculus. Where possible we endeavour to supply key results as they arise.

<hr class="with-margin">
<h4 class="header" id="references">References</h4>

<a name="prml"></a>
* Bishop, C. (2006). [Pattern Recognition and Machine Learning](https://www.springer.com/gb/book/9780387310732)

<a name="esl"></a>
* Hastie, T., R. Tibshirani, and J. Friedman (2001). [The Elements of Statistical Learning](http://web.stanford.edu/~hastie/ElemStatLearn/)

<hr class="with-margin">
