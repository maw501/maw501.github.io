---
layout: post
title: Conceptual questions reviewing MIT 18.06 (Linear Algebra) Unit III
date: 2018-09-19
use_math: true
---

This page is for my conceptual review of the above course, OCW scholar version, link [here](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/). 

The section will be continually added to in light of applications I come across, most likely with relation to machine learning. The idea is not to be exhaustive in explaining and repeating content that is well documented elsewhere, but rather to provide a personal set of mental hooks for my own use. I expect the interest and application for others to be close to zero.

## Questions reviewing Unit III: Positive Definite Matrices and Applications

##### Explain the big deal about positive definite matrices

##### What is diagonalization and why is this so important?

Bluntly it means being able to write a matrix $A$ as $A = S\Lambda S^{-1}$. 

This matters because diagonal matrices are very easy to work with. Their determinant is simply the products of the diagonal entries, they are symmetric and taking powers of them is as simple as taking the powers of the entries.

But more, multiplying a vector (or even a matrix!) by a diagonal matrix is easy:

Multiplying an n-by-n matrix A from the left with diag(a1, ..., an) amounts to multiplying the ith row of A by ai for all i; multiplying the matrix A from the right with diag(a1, ..., an) amounts to multiplying the ith column of A by ai for all i. 

##### Why is the SVD such a big deal (explain conceptually, not what it is)?



##### How do we know if we have a positive definite matrix?


##### Is there a preference for which types of matrices we like?


##### I've heard about similar matrices, what on earth is that all about? I know it relates to the Jordan form


##### Someone mentioned the derivative is a linear operator, what do they mean?


##### Can we use matrices to change the bases in which we operate and if so, how does that work?

