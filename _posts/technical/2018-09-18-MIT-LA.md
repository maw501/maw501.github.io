---
layout: post
title: Conceptual questions reviewing MIT 18.06 (Linear Algebra)
date: 2018-09-19
use_math: true
---

This page is for my conceptual review of the above course, OCW scholar version, link [here](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/).

The section will be continually added to in light of applications I come across, most likely with relation to machine learning. The idea is not to be exhaustive in explaining and repeating content that is well documented elsewhere, but rather to provide a personal set of mental hooks for my own use. I expect the interest and application for others to be close to zero.

**Note**: content needs organising, below is a work in progress. It's also going to be long...very long.

## Questions reviewing Unit I: Ax = b and the 4 fundamental subspaces

#### Explain the 4 fundamental subspaces for a matrix and their dimensions

Pending

#### Explain how the 4 fundamental subspaces are tied to Ax = b

Pending

#### How are the column and row space of a matrix related?

TLDR: every vector in the row space when multiplied by $A$ goes into the column space of $A$. This is because any vector $\textbf{x}$ in $\mathbb{R}^n$ can be written as a component in the row space and a component in the nullspace, i.e. $\textbf{x} = \textbf{x}_r + \textbf{x}_n$. Multiplying by $A$ sends the nullspace component to 0 and leaves $A\textbf{x}_r  = A\textbf{x}$ as required.

**Note 1**: there is an $r$ by $r$ invertible matrix hiding inside any $A$ if we throw away the two nullspaces. In other words, we can go from the row to the column space and back (i.e. make this reduced form of $A$ invertible).

**Note 2**: when we say 'every vector in the row space of $A$' we don't mean that the vector is actually a row in $A$ but rather it's some linear combination of the rows. Since the vector $\textbf{x}$ in $A\textbf{x}$ must have $n$ elements it could alternately be thought of as a candidate for the row space of $A$ (whose elements are in $\mathbb{R}^n$).

## Questions reviewing Unit II: Least Squares, Determinants and Eigenvalues


#### Vector projections onto a subspaces


#### Application to least Squares


## Questions reviewing Unit III: Positive Definite Matrices and Applications


#### Explain the big deal about positive definite matrices

These are symmetric matrices with all eigenvalues $>0$.

#### What's the intuitive link between eigenvectors, change of basis and linear transformations?

**TLDR**: Multiplying by a matrix $A$ is analogous to applying a linear transformation $T$ to a vector, $v$. In 2D in goes $v$ and out pops $T(v) = Av$ and the vector is usually moved somewhere in the 2D plane. The vectors that don't get moved off their line by this linear transformation are the eigenvectors (i.e. they are special vectors that happen to be 'pointing' in the direction of the transformation, they just get scaled by their eigenvalues). If we change bases so our matrix $A$ is now written as $A = S \Lambda S^{-1}$ then the matrix $T$ is now $\Lambda$ and we are thinking of our coordinate system as using the eigenvectors as the basese. This is a more natural setting for the transformation $T$. Here $S$ is a matrix containing the eigenvectors of $A$ as its columns.

*Note*: Read $A = S \Lambda S^{-1}$ as $A$ now consisting of 3 components, applied to the identity matrix, say...first we change basis to our eigenvector basis, then we apply the transformation, then we switch back basis to $A$'s basis.

#### Example of change of basis: how do we change from the cartesian basis $W$ with $w_{1}=(1, 0), w_{2} =(0,1)$ to the basis $V$ with $v_{1}=(3, 7), v_{2}=(2,5)$?

TLDR: expressing $w_{1}$ in the basis $V$ amounts to finding coefficients $c_1$ and $c_2$ such that $w_{1} = c_1 v_1 + c_2 v_2$. This is the same as solving:

$$\begin{bmatrix}
          3 & 2  \\
          7 & 5
          \end{bmatrix}
\begin{bmatrix}
          c_1  \\
          c_2
          \end{bmatrix} =
\begin{bmatrix}
          1  \\
          0
          \end{bmatrix}$$

which is $$\begin{bmatrix}
          v_1 & v_2
          \end{bmatrix}
\begin{bmatrix}
          c_1  \\
          c_2
          \end{bmatrix} = w_1$$

We can solve this in the usual way using an augmented matrix and elimination.

Note that here $T(w) = w$ (i.e. we are not applying any transformation, just a change of basis). We put in a vector and get the same vector out, just expressed in terms of another basis.

Alternate summary: Let the columns of the matrix $W$ be the basis vectors of a new basis. Then if $\textbf{x}$ is a vector in the old basis we can convert it to a vector $c$ in the new basis using $\textbf{x} = W\textbf{c}$. i.e. we have to invert the $W$ change of basis matrix and left multiply by it to get $c$.

##### Longer explanation

It's easier to think of what happens if we try to go from the basis $V$ to $W$. Our change of basis matrix, $M$, is then:

$$M = \begin{bmatrix}
          3 & 2  \\
          7 & 5
          \end{bmatrix}
$$

Why? Well, column 1 of $M$ tells us where $v_1$ ends up in the $W$ basis $\implies$ it ends up at $3w_1 + 7w_2 = v_1$.

Written formally this means:
which is the same as
$$\begin{bmatrix}
          w_1 & w_2
          \end{bmatrix}
M
=
\begin{bmatrix}
          v_1 & v_2
          \end{bmatrix}
$$

So this is the $V$ basis as some linear combination of the $W$ basis vectors, in other words to go from $V$ to $W$ we multiply by $M$!

How do we get back? You guessed it. Right multiply by $M^{-1}$.

**Learning point:** The above dealt with just changing bases, the transformation applied was the identity transformation. We can also do everything together...i.e. take a vector in the cartesian basis, change to a nicer basis (e.g. its eigenbasis), carry out some computation and then change back to the original basis. Computations are usually easier when we choose a nice basis.

For visual awesomeness checkout the below video:

[![Change of basis](http://img.youtube.com/vi/P2LTAUO1TdA/0.jpg)](http://www.youtube.com/watch?v=P2LTAUO1TdA)

Further reading [here](https://eli.thegreenplace.net/2015/change-of-basis-in-linear-algebra/).

#### A note on the change of basis matrix

The change of coordinates matrix from a basis $A$ to $B$ is given by:

$$P = \begin{bmatrix}
          a & c  \\
          b & d
          \end{bmatrix}
$$

which maps us from $A$ to $B$. i.e. $[v]\_B = P[v]\_A$ where $[v]\_B$ reads as 'the vector $v$ expressed in basis $B$'.


When we say coordinates what we really mean are the 'loadings' onto each dimension in the basis. So coordinates $(3, 2)$ in a basis $(x, x^2)$ would correspond to $3x + 2x^2$. The change of basis matrix implicitly assumes this.

For example: suppose we are in 3 dimensions and we have two vectors whose span give a plane (this is the most they can do if they are linearly independent). Let's call them $v_1$ and $v_2$. Then let's also call $B = {v_1, v_2}$ the basis for our plane. Now let's take a vector $a$ which we know sits on the plane (i.e. is in the span of $v_1$ and $v_2$). Suppose further that all we know about $a$ is the coordinates of $a$ in the basis $B$ and $[a]\_B = (7, -4)$. This is straightforward and gives $a = 7v_1 - 4v_2$.

#### Example of functions being the basis and the derivative as the transformation.

TLDR: If we start in the basis with polynomials of degree 3 then our basis is ${1, x, x^2, x^3}$ i.e. every degree 3 polynomial can be expressed as a linear combination of these basis vectors. The derivative operator, call it $T$, when applied to an order 3 polynomial actually maps us to the space of polynomials of degree 2. In this sense the derivative is a linear operator.

Note: Polynomials are really an infinite basis

#### Q: change of basis (fall 08 Q8 final exam)

Consider functions of the form $f(x) = c_1 + c_2 e^x + c_3e^{2x}$ that form a 3 dimensional vector space, $V$.

i) The transformation $d/dx$ can be written as a 2x3 matrix when the domain is specified to have the basis $ \\{ 1, e^x, e^{2x} \\} $. Write down this 2x3 matrix.

Answer: I'll just state the answer and then talk about it:

$$\begin{bmatrix}
          0 & 1 & 0  \\
          0 & 0 & 2
          \end{bmatrix}
$$

Why? Firstly, we are going from a 3d basis to a 2d basis, so our matrix is 2x3 (i.e. it takes a vector with 3 elements in and produces 2 elements).

What are these elements? They are not the basis (that is implicitly assumed)! They are the $c_i$ terms for the basis, or the 'coordinates'.

The matrix above reads like this:

![]({{ site.baseurl }}/images/change_of_basis.jpg)

#### How are positive definite (PD) matrices related to the Hessian matrix?

**TLDR**: Hessian matrix at a critical point gives us information about the local curvature (i.e. how the gradient is changing at that point). Calculating whether the Hessian is PD or not at this point can be used to determine if we are at a minima, maxima or saddle point. See
[here](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/quadratic-approximations/a/quadratic-approximation) and related content for more details and great visualisations.

##### Longer summary

Suppose we have a matrix $A$:

$$A = \begin{bmatrix}
          2 & 6  \\
          6 & 20
          \end{bmatrix}$$

which relates to the function $f(\textbf{x}) = 2x^2 + 12xy + 20y^2$, i.e. this can be written as $\textbf{x}^T A \textbf{x}$ for $\textbf{x}^T = (x, y)$.

Recall that positive definite means that $\textbf{x}^T A \textbf{x} > 0$ for any non-zero $\textbf{x}$. And PD matrices have all eigenvalues positive.

What is the graph of this function (in 3D)? Where are its critical points? Clearly it goes through the origin, but what else?

It turns out that answering this question makes use of PD matrices with a nice helping of calculus:

* We can use calculus to find the first and second derivatives (the Jacobian and Hessian, call $J$ and $H$)
  * The Hessian being positive definite means that $\textbf{x}^T H \textbf{x} > 0$ for any non-zero $\textbf{x}$.
  * i.e. no matter where we evaluate at every set of coordinates $(x, y)$ means the second derivative is positive everywhere $\implies$ our function $f(x,y)$ has positive curvature for every point (i.e. our function is convex).
  * **KEY POINT**: Usually we are most interested in just testing whether $H$ is PD at critical points. This
  [link](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/optimizing-multivariable-functions/a/reasoning-behind-the-second-partial-derivative-test) has more details. This point caused me some confusion, so here is my summary:

    * In a multivariate setting we can approximate a function around a point (say, $\textbf{x}_0$ a critical point) with a quadratic expansion using a Taylor Series expansion.
    * This expansion now has $f_{xx}, f_{yy}$ and $f_{xy}$ terms.
    * All the first derivative terms are 0 in the expansion (we are at a critical point) so our approximation reduces to $f(\textbf{x}) = f(\textbf{x}_0) + \dfrac{1}{2}(\textbf{x}-\textbf{x}_0)^T H f(\textbf{x}_0) (\textbf{x}-\textbf{x}_0)$
    * So testing whether our function goes up or down from the point $\textbf{x}_0$ amounts to testing if the term involving the Hessian is positive or negative.
    * This test amounts to whether $H$ is PD at $\textbf{x}_0$. PD is kind of extending the concept of positive and negative to matrices.
     * This point was confusing me because we've really refined the definition of PD here for the Hessian. PD means for **any** vector whereas we are sat at a critical point and are just testing if the quantity $\dfrac{1}{2}(\textbf{x}-\textbf{x}_0)^T H f(\textbf{x}_0) (\textbf{x}-\textbf{x}_0)$ ends up positive or negative for the single point $\textbf{x}_0$.


* We could also complete the square to obtain: $ 2(x+3y)^2 + 2y^2$
  * Both these terms are always positive and our function is increasing for all $(x, y$) as both get either postively/negatively large.
* We conclude our function looks like a bowl in 3D.

Q: What if the Hessian is PD for all $(x, y)$ but has negative entries?

Q:

#### Linear transformations: idea

Applying a transformation then adding the result is the same as adding the inputs then applying the transformation. Also multiplying by a constant then transforming is the same as transforming then multiplying by a constant.

#### Linear transformations: Which matrix rotates every in 2D by an angle $\theta$?

$$T = \begin{bmatrix}
        \cos \theta & -\sin \theta  \\
        \sin \theta  & \cos \theta
        \end{bmatrix}$$

**Why?** A big insight into linear transformations is that to see what happens to any arbitrary vector (or input, need not be a vector) in the plane we can just follow what happens to the basis vectors. So for the above transformation matrix $T$ the first cartesian basis vector is the $x$ axis with coordinates $v_1 = (1, 0)$ and rotating this vector by an angle $\theta$ lands up at a point $(\cos \theta , \sin \theta )$. Similarly $v_2 = (0, 1)$ ends up at $(-\sin \theta , \cos \theta )$. These new locations get put into the columns of a matrix and to now work out where **any** vector in 2D ends up we simply multiply by $T$!

##### Why does this work?

Simply because the matrix $T$ is a linear transformation so any vector $d$ we had previously could be expressed as $d = c_1 v_1 + c_2 v_2$ and linearity gives us $T(d) = c_1 T(v_1) + c_2 T(v_2)$

#### What is diagonalization and why is this so important?

Bluntly it means being able to write a matrix $A$ as $A = S\Lambda S^{-1}$.

This matters because diagonal matrices are very easy to work with. Their determinant is simply the products of the diagonal entries, they are symmetric and taking powers of them is as simple as taking the powers of the entries.

But more, multiplying a vector (or even a matrix!) by a diagonal matrix is easy. For a matrix $A$ and diagonal matrix $D$:

$D A$ $\implies$ rows of $A$ are scaled by corresponding $D$ entry

$A D$ $\implies$ columns of $A$ are scaled by corresponding $D$ entry


#### Why is the SVD such a big deal (explain conceptually, not what it is)?

The SVD combines many of the big ideas of linear algebra and whilst it's possible to get an understanding, or even use an SVD decomposition it really helps to have got a grasp on some of the linear algebra insights along the way.



#### How do we know if we have a positive definite matrix?


#### Is there a preference for which types of matrices we like?


#### I've heard about similar matrices, what on earth is that all about? I know it relates to the Jordan form




#### Can we use matrices to change the bases in which we operate and if so, how does that work?
