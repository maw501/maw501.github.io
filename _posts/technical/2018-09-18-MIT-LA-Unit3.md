---
layout: post
title: Conceptual questions reviewing MIT 18.06 (Linear Algebra) Unit III
date: 2018-09-19
use_math: true
---

This page is for my conceptual review of the above course, OCW scholar version, link [here](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/).

The section will be continually added to in light of applications I come across, most likely with relation to machine learning. The idea is not to be exhaustive in explaining and repeating content that is well documented elsewhere, but rather to provide a personal set of mental hooks for my own use. I expect the interest and application for others to be close to zero.

## Questions reviewing Unit III: Positive Definite Matrices and Applications

##### Explain the big deal about positive definite matrices

These are symmetric matrices with all eigenvalues $>0$.

##### How are positive definite (PD) matrices related to the Hessian matrix?

Suppose we have a matrix $A$:

$$A = \begin{bmatrix}
          2 & 6  \\
          6 & 20
          \end{bmatrix}$$

which relates to the function $f(\textbf{x}) = 2x^2 + 12xy + 20y^2$.

This can be written as $\textbf{x}^T A \textbf{x}$ for $\textbf{x} = (x, y)$.

What is the graph of this function (in 3D)? Where are its critical points? Clearly it goes through the origin, but what else?

It turns out that answering this question makes use of PD matrices with a nice helping of calculus:

* We can use calculus to find the first and second derivatives (the Jacobian and Hessian, call $J$ and $H$)
  * The Hessian being positive definite at every set of coordinates $(x, y)$ means the second derivative is positive everywhere $\implies$ our function $f(x,y)$ has positive curvature for every point (i.e. our function is convex). Usually we are most interested in just testing whether $H$ is PD at critical points.
* We could also complete the square to obtain: $ 2(x+3y)^2 + 2y^2$
  * Both these terms are always positive and our function is increasing for all $(x, y$) as both get either postively/negatively large.
* We conclude our function looks like a bowl in 3D.

Q: what if the Hessian is PD for all $(x, y)$ but has negative entries?

Q: 

##### What is diagonalization and why is this so important?

Bluntly it means being able to write a matrix $A$ as $A = S\Lambda S^{-1}$.

This matters because diagonal matrices are very easy to work with. Their determinant is simply the products of the diagonal entries, they are symmetric and taking powers of them is as simple as taking the powers of the entries.

But more, multiplying a vector (or even a matrix!) by a diagonal matrix is easy. For a matrix $A$ and diagonal matrix $D$:

$D A$ $\implies$ rows of $A$ are scaled by corresponding $D$ entry

$A D$ $\implies$ columns of $A$ are scaled by corresponding $D$ entry


##### Why is the SVD such a big deal (explain conceptually, not what it is)?



##### How do we know if we have a positive definite matrix?


##### Is there a preference for which types of matrices we like?


##### I've heard about similar matrices, what on earth is that all about? I know it relates to the Jordan form


##### Someone mentioned the derivative is a linear operator, what do they mean?


##### Can we use matrices to change the bases in which we operate and if so, how does that work?
